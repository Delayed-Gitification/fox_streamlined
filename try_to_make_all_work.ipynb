{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2ec3b-3748-47b8-9acc-d44e7dc783a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up AI model...\n",
      "Using device: cpu\n",
      "Model 'wkcn/TinyCLIP-ViT-39M-16-Text-19M-YFCC15M' loaded and ready.\n",
      "‚ö°Ô∏è Quickly checking last known IP: 192.168.68.102...\n",
      "‚úÖ Success! Camera found at the last known IP.\n",
      "Successfully found camera. Connecting to 192.168.68.102...\n",
      "192.168.68.102\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, time, numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Image as IPyImage\n",
    "import threading\n",
    "from queue import Queue\n",
    "import socket\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "import socket\n",
    "import threading\n",
    "from queue import Queue\n",
    "import time\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import hid\n",
    "import time\n",
    "\n",
    "# --- Configuration for your specific relay ---\n",
    "VENDOR_ID = 0x16c0 \n",
    "PRODUCT_ID = 0x05df\n",
    "\n",
    "max_crop_fraction = 0.15  # reject v large areas of motion\n",
    "\n",
    "# The relay number you want to control (e.g., 1 for the first relay; try 0 if 1 fails)\n",
    "RELAY_TO_CONTROL = 1\n",
    "\n",
    "# --- Commands derived from the C code you provided ---\n",
    "# The first byte is the Report ID (always 0x00 for this device).\n",
    "# The second byte is the main command (0xFF for ON, 0xFD for OFF).\n",
    "# The third byte is the relay number to control.\n",
    "RELAY_ON_COMMAND = [0x00, 0xFF, RELAY_TO_CONTROL, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00] \n",
    "RELAY_OFF_COMMAND = [0x00, 0xFD, RELAY_TO_CONTROL, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00]\n",
    "\n",
    "\n",
    "### --- MODIFICATION 1 of 3: Create a Lock --- ###\n",
    "# This lock ensures that only one sprinkler thread can run at a time,\n",
    "# preventing conflicts if new detections occur while the sprinkler is already on.\n",
    "sprinkler_lock = threading.Lock()\n",
    "\n",
    "\n",
    "# --- Main Program ---\n",
    "### --- MODIFICATION 2 of 3: Make the function thread-safe --- ###\n",
    "def sprinkler_on(t=30):\n",
    "    \"\"\"\n",
    "    Activates the sprinkler relay for a given duration. This function is\n",
    "    designed to be run in a separate thread and is thread-safe.\n",
    "    \"\"\"\n",
    "    # Try to acquire the lock without blocking. If another sprinkler operation\n",
    "    # is already running, this will fail, and the function will exit immediately.\n",
    "    if not sprinkler_lock.acquire(blocking=False):\n",
    "        print(\"-> Sprinkler command ignored: An operation is already in progress.\")\n",
    "        return\n",
    "\n",
    "    device = None\n",
    "    try:\n",
    "        print(f\"-> Thread started: Opening device with VID=0x{VENDOR_ID:04x} PID=0x{PRODUCT_ID:04x}\")\n",
    "        device = hid.device()\n",
    "        device.open(VENDOR_ID, PRODUCT_ID)\n",
    "        print(\"-> Device opened successfully.\")\n",
    "        \n",
    "        print(f\"-> Sending ON command to relay #{RELAY_TO_CONTROL} for {t} seconds...\")\n",
    "        device.send_feature_report(RELAY_ON_COMMAND)\n",
    "        time.sleep(0.1)  # Let device process\n",
    "        print(\"-> Relay should be ON.\")\n",
    "        \n",
    "        # Wait for the specified duration\n",
    "        time.sleep(t)\n",
    "        \n",
    "        print(f\"-> Sending OFF command to relay #{RELAY_TO_CONTROL}...\")\n",
    "        device.send_feature_report(RELAY_OFF_COMMAND)\n",
    "        time.sleep(0.1)\n",
    "        print(\"-> Relay should be OFF.\")\n",
    "    \n",
    "    except OSError as e:\n",
    "        time.sleep(t)\n",
    "        print(f\"-> HID error (likely read/write timeout): {e}\")\n",
    "        print(\"-> Device might not support reads‚Äîcommands still sent. Check relay physically.\")\n",
    "    except Exception as e:\n",
    "        time.sleep(t)\n",
    "        print(f\"-> Other error: {e}\")\n",
    "        print(\"-> Run as admin if needed (Jupyter: restart kernel in admin cmd).\")\n",
    "    \n",
    "    finally:\n",
    "        if device:\n",
    "            device.close()\n",
    "            print(\"-> Device closed.\")\n",
    "        # CRUCIAL: Release the lock so that the next sprinkler command can run.\n",
    "        sprinkler_lock.release()\n",
    "        print(\"-> Sprinkler thread finished and lock released.\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# TINYCLIP MODEL SETUP\n",
    "# ==============================================================================\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# --- Configuration ---\n",
    "# This ID tells the script which specific TinyCLIP model to download from Hugging Face.\n",
    "MODEL_ID = \"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\"\n",
    "MODEL_ID = \"wkcn/TinyCLIP-ViT-39M-16-Text-19M-YFCC15M\"\n",
    "\n",
    "# --- Device Selection ---\n",
    "# This automatically checks for the best available hardware. It will use an\n",
    "# NVIDIA GPU (cuda) if available, otherwise it will fall back to your CPU.\n",
    "print(\"Setting up AI model...\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Load Model and Processor ---\n",
    "# The Processor prepares images and text into the numerical format the model needs.\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "# The Model is the actual neural network that performs the scoring.\n",
    "model = CLIPModel.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Move the model onto the selected device (GPU or CPU) and set it to evaluation mode.\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model '{MODEL_ID}' loaded and ready.\")\n",
    "\n",
    "# First 6 are people\n",
    "n_people_decoys = 6\n",
    "PROMPTS = [\n",
    "    \"man\", \"woman\", \"a person walking\", \"a photograph of a person\",\n",
    "    \"people\", \"person wearing a jumper\", \"a photograph of a fox\",\n",
    "    \"a photograph of a frog\", \"an empty garden at night\", \"a car\", 'grass', 'empty', 'nothing', 'plants',\n",
    "    'blurry image of nothing', 'a photograph of a cat', 'field', 'farm', 'desert',\n",
    "    \"a blurry shadow moving on the ground\", \"a blurry shape moving quickly\", \"an alien\",\n",
    "    \"overexposed image of a person walking\"\n",
    "]\n",
    "\n",
    "FOX_SCORE_THRESHOLD = 0.40 # Set a confidence threshold (e.g., 60%)\n",
    "people_sum_thresh = 0.3 # must be less than 0.3\n",
    "\n",
    "def _check_port(ip, port, timeout=0.5):\n",
    "    \"\"\"A quick, private helper function to check if a single port is open.\"\"\"\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.settimeout(timeout)\n",
    "    try:\n",
    "        # connect_ex returns 0 if the connection is successful (port is open)\n",
    "        if sock.connect_ex((ip, port)) == 0:\n",
    "            return True\n",
    "    except socket.error:\n",
    "        # This can happen for various network reasons (e.g., host unreachable)\n",
    "        return False\n",
    "    finally:\n",
    "        sock.close()\n",
    "    return False\n",
    "\n",
    "def find_camera_ip(default_ip='192.168.68.102', subnet='192.168.68', port=554):\n",
    "    \"\"\"\n",
    "    Finds the camera IP. First, it quickly checks a default/last-known IP.\n",
    "    If that fails, it performs a full, slower scan of the subnet.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Fast Check ---\n",
    "    # Try the provided default_ip first for a near-instant startup.\n",
    "    if default_ip:\n",
    "        print(f\"‚ö°Ô∏è Quickly checking last known IP: {default_ip}...\")\n",
    "        if _check_port(default_ip, port):\n",
    "            print(\"‚úÖ Success! Camera found at the last known IP.\")\n",
    "            return default_ip\n",
    "        else:\n",
    "            print(f\"‚ùå Last known IP failed. Starting a full network scan...\")\n",
    "\n",
    "    # --- Step 2: Full Scan (Fallback) ---\n",
    "    # This part only runs if the fast check above fails.\n",
    "    print(f\"üê¢ Scanning subnet {subnet}.x for a camera on port {port}...\")\n",
    "    for i in range(1, 255):\n",
    "        ip = f\"{subnet}.{i}\"\n",
    "        print(f\"\\rChecking: {ip}  \", end=\"\")\n",
    "        if _check_port(ip, port):\n",
    "            print(f\"\\n‚úÖ Found new camera IP at: {ip}\")\n",
    "            return ip\n",
    "\n",
    "    print(\"\\n‚ùå Camera not found on this subnet after full scan.\")\n",
    "    return None\n",
    "# =========================\n",
    "# Camera config (edit here)\n",
    "# =========================\n",
    "USER = \"foxyfoxy\"\n",
    "PASS = \"foxyfoxy123\"\n",
    "\n",
    "# Find the IP automatically by providing the known subnet\n",
    "IP = find_camera_ip(subnet='192.168.68')\n",
    "\n",
    "# Check the result and proceed\n",
    "if IP:\n",
    "    print(f\"Successfully found camera. Connecting to {IP}...\")\n",
    "    RTSP_URL = f\"rtsp://{USER}:{PASS}@{IP}:554/stream1\"\n",
    "    # --- Put the rest of your camera logic here ---\n",
    "else:\n",
    "    print(\"Could not find the Tapo camera. Check its connection and the subnet.\")\n",
    "print(IP)\n",
    "\n",
    "RTSP_LOW  = f\"rtsp://{USER}:{PASS}@{IP}:554/stream2\"  # motion + lookback (LOW)\n",
    "RTSP_HIGH = f\"rtsp://{USER}:{PASS}@{IP}:554/stream1\"  # HQ \"now\" for crops\n",
    "\n",
    "# Keep it reliable (some builds ignore extras)\n",
    "os.environ[\"OPENCV_FFMPEG_CAPTURE_OPTIONS\"] = \"rtsp_transport;tcp|max_delay;0\"\n",
    "# Enhanced for Windows: single FFmpeg thread + low delay\n",
    "os.environ[\"OPENCV_FFMPEG_CAPTURE_OPTIONS\"] = \"rtsp_transport;tcp|max_delay;0|threads;1\"\n",
    "# =========================\n",
    "# ROI mask (normalized polygon)\n",
    "# =========================\n",
    "USE_MASK = True\n",
    "MASK_POLYGONS = [[\n",
    "    (0.0, 1.0),   # bottom-left\n",
    "    (0.85, 1.0),  # bottom-right\n",
    "    (0.98, 0.4),  # top-right\n",
    "    (0.7, 0.35),\n",
    "    (0.0, 0.57)   # top-left\n",
    "]]\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "\n",
    "def ensure_dir(p):\n",
    "    if not os.path.isdir(p):\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def timestamp_str(ts=None):\n",
    "    if ts is None: ts = time.time()\n",
    "    return datetime.fromtimestamp(ts).strftime(\"%Y%m%d_%H%M%S_%f\")[:-3]\n",
    "\n",
    "def make_mask(shape):\n",
    "    \"\"\"Return uint8 mask {0,255} for given frame shape using normalized polygon(s).\"\"\"\n",
    "    if not USE_MASK or not MASK_POLYGONS:\n",
    "        return np.full(shape[:2], 255, dtype=np.uint8)\n",
    "    h, w = shape[:2]\n",
    "    m = np.zeros((h, w), dtype=np.uint8)\n",
    "    for poly in MASK_POLYGONS:\n",
    "        pts = np.array([[int(x*w), int(y*h)] for x, y in poly], dtype=np.int32)\n",
    "        cv2.fillPoly(m, [pts], 255)\n",
    "    return m\n",
    "\n",
    "def grab_latest_nonblocking(cap, max_ms=8, max_grabs=60):  # Bumped max_grabs for better drain\n",
    "    \"\"\"Drain queued frames fast, then retrieve newest without blocking too long.\"\"\"\n",
    "    t0 = time.time()\n",
    "    grabs = 0\n",
    "    while grabs < max_grabs:\n",
    "        if not cap.grab():  # nothing ready\n",
    "            break\n",
    "        grabs += 1\n",
    "        if (time.time() - t0) * 1000.0 >= max_ms:\n",
    "            break\n",
    "    ok, frame = cap.retrieve()\n",
    "    if not ok or frame is None:\n",
    "        ok, frame = cap.read()\n",
    "    return ok, frame\n",
    "\n",
    "def encode_jpeg(img, quality=60):\n",
    "    ok, buf = cv2.imencode(\".jpg\", img, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
    "    return buf.tobytes() if ok else None\n",
    "\n",
    "def merge_contours_to_bbox(contours, min_area=150):\n",
    "    \"\"\"Return a single union bbox (x,y,w,h) for all contours above min_area; None if none.\"\"\"\n",
    "    boxes = []\n",
    "    for c in contours:\n",
    "        if cv2.contourArea(c) > min_area:\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            boxes.append((x, y, x+w, y+h))\n",
    "    if not boxes:\n",
    "        return None\n",
    "    x1 = min(b[0] for b in boxes)\n",
    "    y1 = min(b[1] for b in boxes)\n",
    "    x2 = max(b[2] for b in boxes)\n",
    "    y2 = max(b[3] for b in boxes)\n",
    "    return (x1, y1, x2 - x1, y2 - y1)\n",
    "\n",
    "def pad_and_clip_rect(x, y, w, h, pad_px, W, H):\n",
    "    x = max(0, int(x - pad_px)); y = max(0, int(y - pad_px))\n",
    "    w = int(w + 2*pad_px);       h = int(h + 2*pad_px)\n",
    "    x2 = min(W, x + w);          y2 = min(H, y + h)\n",
    "    return x, y, x2 - x, y2 - y\n",
    "\n",
    "def choose_best_by_time(history, target_ts, max_age=None):\n",
    "    \"\"\"\n",
    "    history: deque of (ts, frame). Returns (ts, frame) with |ts-target_ts| minimal.\n",
    "    If max_age provided (seconds), require |dt| <= max_age else return None.\n",
    "    \"\"\"\n",
    "    best = None; best_dt = 1e9\n",
    "    for ts, f in history:\n",
    "        dt = abs(ts - target_ts)\n",
    "        if dt < best_dt:\n",
    "            best_dt = dt\n",
    "            best = (ts, f)\n",
    "    if best is None:\n",
    "        return None\n",
    "    if (max_age is not None) and (best_dt > max_age):\n",
    "        return None\n",
    "    return best\n",
    "\n",
    "def update_display(handle, frame_bgr, mask_bin, roi_mask):\n",
    "    \"\"\"Lean UI: draw boxes & show low-res binary side-by-side (no big previews).\"\"\"\n",
    "    vis = frame_bgr.copy()\n",
    "    if roi_mask is not None:\n",
    "        inv = cv2.bitwise_not(roi_mask)\n",
    "        dim = (vis * 0.35).astype(np.uint8)\n",
    "        vis = cv2.add(cv2.bitwise_and(dim, dim, mask=inv),\n",
    "                          cv2.bitwise_and(vis, vis, mask=roi_mask))\n",
    "    contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for c in contours:\n",
    "        if cv2.contourArea(c) > 600:\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            cv2.rectangle(vis, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "    right = cv2.cvtColor(mask_bin, cv2.COLOR_GRAY2BGR)\n",
    "    panel = np.hstack([vis, right])\n",
    "    data = encode_jpeg(panel, quality=55)  # smaller UI payload\n",
    "    if data is not None:\n",
    "        handle.update(IPyImage(data=data))\n",
    "\n",
    "# =========================\n",
    "# Background HQ sampler (non-blocking)\n",
    "# =========================\n",
    "class HQSampler:\n",
    "    def __init__(self, rtsp_url):\n",
    "        self.rtsp = rtsp_url\n",
    "        self.cap = cv2.VideoCapture(self.rtsp, cv2.CAP_FFMPEG)\n",
    "        if not self.cap.isOpened():\n",
    "            raise RuntimeError(\"Could not open HQ RTSP stream\")\n",
    "        try: self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "        except Exception: pass\n",
    "        self.latest = (0.0, None)   # (ts, frame)\n",
    "        self.alive = True\n",
    "        self.lock = threading.Lock()\n",
    "        self.t = threading.Thread(target=self._loop, daemon=True)\n",
    "        self.t.start()\n",
    "\n",
    "    def _loop(self):\n",
    "        while self.alive:\n",
    "            ok, frame = grab_latest_nonblocking(self.cap, max_ms=8, max_grabs=60)  # Bumped\n",
    "            if ok and frame is not None:\n",
    "                with self.lock:\n",
    "                    self.latest = (time.time(), frame)\n",
    "            else:\n",
    "                # attempt quick reopen\n",
    "                try: self.cap.release()\n",
    "                except: pass\n",
    "                time.sleep(0.1)  # Shorter sleep for faster reconnect\n",
    "                self.cap = cv2.VideoCapture(self.rtsp, cv2.CAP_FFMPEG)\n",
    "                try: self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "                except: pass\n",
    "\n",
    "    def get_latest(self, max_age=1.0):  # Tighter max_age for fresher HQ\n",
    "        with self.lock:\n",
    "            ts, fr = self.latest\n",
    "        if fr is None or (time.time() - ts) > max_age:\n",
    "            return None\n",
    "        return fr\n",
    "\n",
    "    def stop(self):\n",
    "        self.alive = False\n",
    "        try: self.t.join(timeout=0.5)\n",
    "        except: pass\n",
    "        try: self.cap.release()\n",
    "        except: pass\n",
    "\n",
    "# =========================\n",
    "# Async file writer (non-blocking saves)\n",
    "# =========================\n",
    "class FileWriter:\n",
    "    def __init__(self, maxsize=32):\n",
    "        self.q = Queue(maxsize=maxsize)\n",
    "        self.t = threading.Thread(target=self._loop, daemon=True)\n",
    "        self.t.start()\n",
    "\n",
    "    def _loop(self):\n",
    "        while True:\n",
    "            item = self.q.get()\n",
    "            if item is None: break\n",
    "            path, img, quality = item\n",
    "            try:\n",
    "                cv2.imwrite(path, img, [int(cv2.IMWRITE_JPEG_QUALITY), int(quality)])\n",
    "            except Exception:\n",
    "                pass\n",
    "            self.q.task_done()\n",
    "\n",
    "    def save_jpg(self, path, img, quality=82):\n",
    "        try:\n",
    "            self.q.put_nowait((path, img, quality))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def stop(self):\n",
    "        try: self.q.put(None)\n",
    "        except: pass\n",
    "        try: self.t.join(timeout=1.0)\n",
    "        except: pass\n",
    "\n",
    "\n",
    "# ===== AI functions ======\n",
    "\n",
    "# This new function takes an image frame (NumPy array) instead of a file path.\n",
    "def score_frame_against_prompts(frame, prompts, processor, model, device):\n",
    "    \"\"\"\n",
    "    Takes an OpenCV frame (NumPy array), runs it through the CLIP model,\n",
    "    and returns the probabilities for each prompt.\n",
    "    \"\"\"\n",
    "    # 1. Convert the color space from OpenCV's BGR to the standard RGB.\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 2. Convert the NumPy array to a Pillow Image object.\n",
    "    image = Image.fromarray(rgb_frame)\n",
    "    \n",
    "    # --- The rest of the logic is the same as your original function ---\n",
    "    \n",
    "    # Preprocess the text and image.\n",
    "    inputs = processor(text=prompts, images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Move the input tensors to the selected device (GPU or CPU).\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Perform inference.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        # The softmax function converts scores into probabilities.\n",
    "        probs = logits_per_image.softmax(dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "    return probs\n",
    "\n",
    "# =========================\n",
    "# Main runner\n",
    "# =========================\n",
    "def run_motion_view(display_width=480, ui_fps=3, lookback_secs=10.0,  # Higher fps for lower delay\n",
    "                    save_dir=\"motion_events\", jpg_quality=82,\n",
    "                    event_cooldown=0.5, timings=False,  # Shorter cooldown if needed\n",
    "                    post_sprinkler_cooldown=5):  # New: secs to ignore motion after sprinkler off\n",
    "    \"\"\"\n",
    "    Optimized: Higher FPS, better drain, post-sprinkler cooldown for false positives.\n",
    "    Bugfix: Removed erroneous 'or decoys > 0.1' trigger.\n",
    "    During sprinkler: Skip motion/score; save low-res; build history.\n",
    "    On events: Show annotated crop image (with key scores overlaid) via display().\n",
    "    \"\"\"\n",
    "    ensure_dir(save_dir)\n",
    "    ensure_dir(\"sprinkler_sessions\")  # Global dir for sprinkler logs\n",
    "    highest_fox_score = 0\n",
    "\n",
    "    # Timings setup (optional)\n",
    "    if timings:\n",
    "        import time as _time\n",
    "        frame_times = deque(maxlen=100)\n",
    "\n",
    "    # Open LOW stream for detection\n",
    "    cap_low = cv2.VideoCapture(RTSP_LOW, cv2.CAP_FFMPEG)\n",
    "    if not cap_low.isOpened():\n",
    "        raise RuntimeError(\"Could not open LOW RTSP stream\")\n",
    "    try: cap_low.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "    except Exception: pass\n",
    "\n",
    "    # Background HQ sampler (kept warm)\n",
    "    hq = HQSampler(RTSP_HIGH)\n",
    "\n",
    "    # Async file writer\n",
    "    fw = FileWriter()\n",
    "\n",
    "    # UI DISABLED: No display handle or updates\n",
    "\n",
    "    roi_mask_low = None\n",
    "\n",
    "    # Histories (LOW)\n",
    "    hist_len = int(max(lookback_secs * ui_fps * 2, 40))\n",
    "    low_gray_history  = deque(maxlen=hist_len)   # (ts, gray_roi_low)\n",
    "    low_color_history = deque(maxlen=hist_len)   # (ts, full_color_low)\n",
    "\n",
    "    interval = 1.0 / float(max(1, ui_fps))\n",
    "    next_tick = time.time()\n",
    "\n",
    "    # Sprinkler session tracking\n",
    "    sprinkler_session_dir = None\n",
    "    prev_sprinkler_locked = False\n",
    "    last_sprinkler_end_ts = 0.0  # For post-cooldown\n",
    "\n",
    "    # Event throttle\n",
    "    last_event_ts = 0.0\n",
    "    print(f\"Processing at ~{ui_fps} FPS (no UI). Scoring crops on motion; console shows TinyCLIP probs‚Ä¶ (interrupt to stop)\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            tick_start = time.time() if timings else None\n",
    "            now = time.time()\n",
    "            # tick pacing\n",
    "            if now < next_tick:\n",
    "                time.sleep(max(0, next_tick - now))\n",
    "                continue\n",
    "            next_tick += interval\n",
    "\n",
    "            # --- LOW: grab newest quickly (always, for history building)\n",
    "            ok_low, frame_low = grab_latest_nonblocking(cap_low, max_ms=8, max_grabs=60)\n",
    "            if not ok_low or frame_low is None:\n",
    "                try: cap_low.release()\n",
    "                except Exception: pass\n",
    "                time.sleep(0.1)  # Shorter reconnect\n",
    "                cap_low = cv2.VideoCapture(RTSP_LOW, cv2.CAP_FFMPEG)\n",
    "                try: cap_low.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "                except Exception: pass\n",
    "                if timings: frame_times.append(time.time() - tick_start)\n",
    "                continue\n",
    "\n",
    "            # Resize LOW for consistent processing (keep low res for speed)\n",
    "            if display_width and frame_low.shape[1] > display_width:\n",
    "                h = int(frame_low.shape[0] * (display_width / float(frame_low.shape[1])))\n",
    "                frame_low = cv2.resize(frame_low, (display_width, h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # ROI mask per current LOW resolution\n",
    "            if roi_mask_low is None:\n",
    "                roi_mask_low = make_mask(frame_low.shape)\n",
    "\n",
    "            # Build grayscale ROI for history (always build/push, even during sprinkler)\n",
    "            gray = cv2.cvtColor(frame_low, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "            gray_roi = cv2.bitwise_and(gray, gray, mask=roi_mask_low)\n",
    "\n",
    "            # Push current LOW frames to histories (always, for post-sprinkler readiness)\n",
    "            low_gray_history.append((now, gray_roi.copy()))\n",
    "            low_color_history.append((now, frame_low.copy()))\n",
    "\n",
    "            # --- Sprinkler state check ---\n",
    "            current_sprinkler_locked = sprinkler_lock.locked()\n",
    "\n",
    "            # Detect transitions\n",
    "            if current_sprinkler_locked and not prev_sprinkler_locked:\n",
    "                print(\"-> Sprinkler activated: Starting low-res logging session.\")\n",
    "            elif not current_sprinkler_locked and prev_sprinkler_locked:\n",
    "                last_sprinkler_end_ts = now  # Start post-cooldown timer\n",
    "                if sprinkler_session_dir:\n",
    "                    print(f\"-> Sprinkler deactivated: Low-res session logged to {sprinkler_session_dir}\")\n",
    "                sprinkler_session_dir = None\n",
    "\n",
    "            # During sprinkler: Save low-res frame (non-blocking)\n",
    "            if current_sprinkler_locked and sprinkler_session_dir:\n",
    "                frame_path = os.path.join(sprinkler_session_dir, f\"low_{timestamp_str(now)}.jpg\")\n",
    "                fw.save_jpg(frame_path, frame_low, quality=70)  # Lower quality for speed/storage\n",
    "\n",
    "            prev_sprinkler_locked = current_sprinkler_locked\n",
    "\n",
    "            # --- Skip motion/score if sprinkler active or in post-cooldown ---\n",
    "            if current_sprinkler_locked or (now - last_sprinkler_end_ts < post_sprinkler_cooldown):\n",
    "                if timings: frame_times.append(time.time() - tick_start)\n",
    "                continue\n",
    "\n",
    "            # Reference LOW ~lookback_secs ago (for diff + crop-then)\n",
    "            ref_ts_target = now - lookback_secs\n",
    "            ref_low_gray = choose_best_by_time(low_gray_history,  ref_ts_target)\n",
    "            ref_low_col  = choose_best_by_time(low_color_history, ref_ts_target, max_age=3.0)\n",
    "\n",
    "            if ref_low_gray is None or ref_low_col is None:\n",
    "                # Not enough history yet; skip\n",
    "                if timings: frame_times.append(time.time() - tick_start)\n",
    "                continue\n",
    "            _, ref_gray = ref_low_gray\n",
    "            _, low_then_frame = ref_low_col\n",
    "\n",
    "            # --- Motion mask (LOW): keep it light (higher thresh for fewer falses)\n",
    "            diff = cv2.absdiff(gray_roi, ref_gray)\n",
    "            _, motion_bin = cv2.threshold(diff, 18, 255, cv2.THRESH_BINARY)  # Bumped thresh\n",
    "            motion_bin = cv2.morphologyEx(motion_bin, cv2.MORPH_OPEN,\n",
    "                                          np.ones((3,3), np.uint8), iterations=1)\n",
    "            motion_bin = cv2.dilate(motion_bin, None, iterations=1)\n",
    "\n",
    "            # --- If motion detected, crop/score/save (cooldown + non-blocking HQ + async writes)\n",
    "            contours, _ = cv2.findContours(motion_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            union_bbox_low = merge_contours_to_bbox(contours, min_area=150)\n",
    "\n",
    "            if union_bbox_low and (union_bbox_low[2] * union_bbox_low[3]) > (frame_low.shape[0] * frame_low.shape[1] * max_crop_fraction):\n",
    "                print(f\"-> Motion rejected: Area too large.\")\n",
    "                union_bbox_low = None\n",
    "\n",
    "            if union_bbox_low is None:\n",
    "                if timings: frame_times.append(time.time() - tick_start)\n",
    "                continue\n",
    "\n",
    "            # Cooldown to avoid floods on continuous motion\n",
    "            if (now - last_event_ts) < float(event_cooldown):\n",
    "                if timings: frame_times.append(time.time() - tick_start)\n",
    "                continue\n",
    "            last_event_ts = now\n",
    "\n",
    "            # Get a recent HQ frame instantly (from sampler)\n",
    "            hq_now = hq.get_latest(max_age=1.0)\n",
    "            if hq_now is None:\n",
    "                if timings: frame_times.append(time.time() - tick_start)\n",
    "                continue\n",
    "\n",
    "            # Map LOW bbox to HQ coords\n",
    "            x, y, w, h = union_bbox_low\n",
    "            Hl, Wl = frame_low.shape[:2]\n",
    "            Hh, Wh = hq_now.shape[:2]\n",
    "            sx = Wh / float(Wl)\n",
    "            sy = Hh / float(Hl)\n",
    "\n",
    "            x_hq = int(x * sx); y_hq = int(y * sy)\n",
    "            w_hq = int(w * sx); h_hq = int(h * sy)\n",
    "\n",
    "            # Pad & clip both\n",
    "            pad_hq = int(0.07 * max(w_hq, h_hq)) + 8\n",
    "            x_hq, y_hq, w_hq, h_hq = pad_and_clip_rect(x_hq, y_hq, w_hq, h_hq, pad_hq, Wh, Hh)\n",
    "\n",
    "            pad_low = int(0.07 * max(w, h)) + 4\n",
    "            xL, yL, wL, hL = pad_and_clip_rect(x, y, w, h, pad_low,\n",
    "                                               frame_low.shape[1], frame_low.shape[0])\n",
    "\n",
    "            # Extract crops\n",
    "            crop_hq_now   = hq_now[y_hq:y_hq+h_hq, x_hq:x_hq+w_hq]\n",
    "            crop_low_then = low_then_frame[yL:yL+hL, xL:xL+wL]\n",
    "\n",
    "            # Score the cropped image against your prompts\n",
    "            event_start = time.time() if timings else None\n",
    "            probabilities = score_frame_against_prompts(crop_hq_now, PROMPTS, processor, model, DEVICE)\n",
    "            if timings: ai_time = time.time() - event_start\n",
    "\n",
    "            # Annotate crop with key scores (for visual \"next to scores\")\n",
    "            annotated = crop_hq_now.copy()\n",
    "            h_img, w_img = annotated.shape[:2]\n",
    "\n",
    "            # Fox score (big green if high)\n",
    "            color_fox = (0, 255, 0) if probabilities[PROMPTS.index(\"a photograph of a fox\")] > FOX_SCORE_THRESHOLD else (255, 255, 255)\n",
    "            cv2.putText(annotated, f\"Fox: {probabilities[PROMPTS.index('a photograph of a fox')]:.3f}\", \n",
    "                        (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color_fox, 2)\n",
    "\n",
    "            # People sum (red if high)\n",
    "            decoys = np.sum(probabilities[:n_people_decoys])\n",
    "            color_people = (0, 0, 255) if decoys > people_sum_thresh else (255, 255, 255)\n",
    "            cv2.putText(annotated, f\"People: {decoys:.3f}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color_people, 2)\n",
    "\n",
    "            # Top 3 prompts (white, smaller)\n",
    "            top3 = sorted(zip(PROMPTS, probabilities), key=lambda x: x[1], reverse=True)[:3]\n",
    "            for i, (prompt, score) in enumerate(top3):\n",
    "                y_pos = 90 + i * 25\n",
    "                cv2.putText(annotated, f\"{prompt[:20]}: {score:.3f}\", \n",
    "                            (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "            # Resize if too wide for display\n",
    "            if w_img > 600:\n",
    "                scale = 600 / w_img\n",
    "                new_w = 600\n",
    "                new_h = int(h_img * scale)\n",
    "                annotated = cv2.resize(annotated, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Encode and display annotated crop\n",
    "            data = encode_jpeg(annotated, 85)\n",
    "            if data:\n",
    "                display(IPyImage(data=data, width=600))  # Inline display, sized\n",
    "\n",
    "            # Full scores output (console below image)\n",
    "            print(\"\\n=== TinyCLIP Scores ===\")\n",
    "            for thing, prob in zip(PROMPTS, probabilities):\n",
    "                print(f\"{thing}: {prob:.3f}\")\n",
    "            print(\"======================\\n\")\n",
    "\n",
    "            # Find the score specifically for the \"fox\" prompt\n",
    "            fox_index = PROMPTS.index(\"a photograph of a fox\")\n",
    "            fox_score = probabilities[fox_index]\n",
    "\n",
    "            decoys = np.sum(probabilities[:n_people_decoys])\n",
    "\n",
    "            print(f\"fox: {fox_score:.3f}, people_sum: {decoys:.3f}, diff: {fox_score - decoys:.3f}\")\n",
    "\n",
    "            if fox_score > highest_fox_score:\n",
    "                print(f\"New high fox score: {fox_score:.3f}\")\n",
    "                highest_fox_score = fox_score\n",
    "\n",
    "            # If the score is above your threshold, take action\n",
    "            if (fox_score > FOX_SCORE_THRESHOLD and decoys < people_sum_thresh):  # Removed buggy 'or decoys > 0.1'\n",
    "                print(f\"ü¶ä Fox detected with confidence {fox_score:.3f}! Triggering water valve... üíß\")\n",
    "                \n",
    "                ### --- Set sprinkler session dir on trigger ---\n",
    "                if sprinkler_session_dir is None:  # Only if not already set (prevents overlap)\n",
    "                    sprinkler_session_dir = os.path.join(\"sprinkler_sessions\", timestamp_str(now))\n",
    "                    ensure_dir(sprinkler_session_dir)\n",
    "                    print(f\"-> Logging low-res to: {sprinkler_session_dir}\")\n",
    "                \n",
    "                ### --- Run sprinkler_on in a thread (skips if already locked) --- ###\n",
    "                # The thread's acquire(blocking=False) will ignore if locked\n",
    "                sprinkler_time = 5\n",
    "                sprinkler_thread = threading.Thread(target=sprinkler_on, args=(sprinkler_time,), daemon=True)\n",
    "                sprinkler_thread.start()\n",
    "                \n",
    "                # Save the image of the detected fox for review\n",
    "                ev_dir = os.path.join(save_dir, timestamp_str(now))\n",
    "                ensure_dir(ev_dir)\n",
    "                # You can give it a special name to easily find it later\n",
    "                fw.save_jpg(os.path.join(ev_dir, f\"fox_detected_{fox_score:.3f}.jpg\"), crop_hq_now, jpg_quality)\n",
    "\n",
    "            if timings:\n",
    "                frame_times.append(time.time() - tick_start)\n",
    "                if 'ai_time' in locals(): print(f\"Frame: {frame_times[-1]:.2f}s, AI: {ai_time:.2f}s\")\n",
    "                if len(frame_times) % 10 == 0:  # Avg every 10 frames (more frequent sans UI)\n",
    "                    print(f\"Avg frame time: {np.mean(frame_times):.2f}s (max {np.max(frame_times):.2f}s)\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopped.\")\n",
    "    finally:\n",
    "        try: cap_low.release()\n",
    "        except: pass\n",
    "        try: hq.stop()\n",
    "        except: pass\n",
    "        try: fw.stop()\n",
    "        except: pass\n",
    "        if timings: print(f\"Final avg frame: {np.mean(frame_times):.2f}s\")\n",
    "        # Cleanup: If session ongoing, note it\n",
    "        if sprinkler_session_dir:\n",
    "            print(f\"-> Interrupt: Low-res session left open at {sprinkler_session_dir}\")\n",
    "\n",
    "# =========================\n",
    "# Run it\n",
    "# =========================\n",
    "# ui_fps controls how often a frame is processed & how densely the LOW history is sampled.\n",
    "# lookback_secs is the comparison window (LOW ~10s ago vs HQ now).\n",
    "run_motion_view(\n",
    "    ui_fps=1,  # Higher for lower detection delay\n",
    "    lookback_secs=10.0,\n",
    "    save_dir=\"motion_events\",\n",
    "    jpg_quality=82,\n",
    "    event_cooldown=0.5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tinyclip_env)",
   "language": "python",
   "name": "tinyclip_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
