{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb6db5-bb5c-4132-b9f7-230b8e10ebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up AI model...\n",
      "Using device: cpu\n",
      "Model 'wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M' loaded and ready.\n",
      "‚ö°Ô∏è Quickly checking last known IP: 192.168.68.102...\n",
      "‚úÖ Success! Camera found at the last known IP.\n",
      "Successfully found camera. Connecting to 192.168.68.102...\n",
      "192.168.68.102\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, time, numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from IPython.display import display, Image as IPyImage\n",
    "import threading\n",
    "from queue import Queue\n",
    "import socket\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "import socket\n",
    "import threading\n",
    "from queue import Queue\n",
    "import time\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# ==============================================================================\n",
    "# TINYCLIP MODEL SETUP\n",
    "# ==============================================================================\n",
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "# --- Configuration ---\n",
    "# This ID tells the script which specific TinyCLIP model to download from Hugging Face.\n",
    "MODEL_ID = \"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\"\n",
    "\n",
    "# --- Device Selection ---\n",
    "# This automatically checks for the best available hardware. It will use an\n",
    "# NVIDIA GPU (cuda) if available, otherwise it will fall back to your CPU.\n",
    "print(\"Setting up AI model...\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Load Model and Processor ---\n",
    "# The Processor prepares images and text into the numerical format the model needs.\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "# The Model is the actual neural network that performs the scoring.\n",
    "model = CLIPModel.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Move the model onto the selected device (GPU or CPU) and set it to evaluation mode.\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model '{MODEL_ID}' loaded and ready.\")\n",
    "\n",
    "# First 6 are people\n",
    "n_people_decoys = 6\n",
    "PROMPTS = [\n",
    "    \"man\", \"woman\", \"a person walking\", \"a photograph of a person\",\n",
    "    \"people\", \"person wearing a jumper\", \"a photograph of a fox\",\n",
    "    \"a photograph of a frog\", \"an empty garden at night\", \"a car\", 'grass', 'empty', 'nothing', 'plants',\n",
    "    'blurry image of nothing', 'a photograph of a cat'\n",
    "]\n",
    "\n",
    "FOX_SCORE_THRESHOLD = 0.40 # Set a confidence threshold (e.g., 60%)\n",
    "people_sum_thresh = 0.3 # must be less than 0.3\n",
    "\n",
    "def _check_port(ip, port, timeout=0.5):\n",
    "    \"\"\"A quick, private helper function to check if a single port is open.\"\"\"\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.settimeout(timeout)\n",
    "    try:\n",
    "        # connect_ex returns 0 if the connection is successful (port is open)\n",
    "        if sock.connect_ex((ip, port)) == 0:\n",
    "            return True\n",
    "    except socket.error:\n",
    "        # This can happen for various network reasons (e.g., host unreachable)\n",
    "        return False\n",
    "    finally:\n",
    "        sock.close()\n",
    "    return False\n",
    "\n",
    "def find_camera_ip(default_ip='192.168.68.102', subnet='192.168.68', port=554):\n",
    "    \"\"\"\n",
    "    Finds the camera IP. First, it quickly checks a default/last-known IP.\n",
    "    If that fails, it performs a full, slower scan of the subnet.\n",
    "    \"\"\"\n",
    "    # --- Step 1: Fast Check ---\n",
    "    # Try the provided default_ip first for a near-instant startup.\n",
    "    if default_ip:\n",
    "        print(f\"‚ö°Ô∏è Quickly checking last known IP: {default_ip}...\")\n",
    "        if _check_port(default_ip, port):\n",
    "            print(\"‚úÖ Success! Camera found at the last known IP.\")\n",
    "            return default_ip\n",
    "        else:\n",
    "            print(f\"‚ùå Last known IP failed. Starting a full network scan...\")\n",
    "\n",
    "    # --- Step 2: Full Scan (Fallback) ---\n",
    "    # This part only runs if the fast check above fails.\n",
    "    print(f\"üê¢ Scanning subnet {subnet}.x for a camera on port {port}...\")\n",
    "    for i in range(1, 255):\n",
    "        ip = f\"{subnet}.{i}\"\n",
    "        print(f\"\\rChecking: {ip}  \", end=\"\")\n",
    "        if _check_port(ip, port):\n",
    "            print(f\"\\n‚úÖ Found new camera IP at: {ip}\")\n",
    "            return ip\n",
    "\n",
    "    print(\"\\n‚ùå Camera not found on this subnet after full scan.\")\n",
    "    return None\n",
    "# =========================\n",
    "# Camera config (edit here)\n",
    "# =========================\n",
    "USER = \"foxyfoxy\"\n",
    "PASS = \"foxyfoxy123\"\n",
    "\n",
    "# Find the IP automatically by providing the known subnet\n",
    "IP = find_camera_ip(subnet='192.168.68')\n",
    "\n",
    "# Check the result and proceed\n",
    "if IP:\n",
    "    print(f\"Successfully found camera. Connecting to {IP}...\")\n",
    "    RTSP_URL = f\"rtsp://{USER}:{PASS}@{IP}:554/stream1\"\n",
    "    # --- Put the rest of your camera logic here ---\n",
    "else:\n",
    "    print(\"Could not find the Tapo camera. Check its connection and the subnet.\")\n",
    "print(IP)\n",
    "\n",
    "RTSP_LOW  = f\"rtsp://{USER}:{PASS}@{IP}:554/stream2\"  # motion + lookback (LOW)\n",
    "RTSP_HIGH = f\"rtsp://{USER}:{PASS}@{IP}:554/stream1\"  # HQ \"now\" for crops\n",
    "\n",
    "# Keep it reliable (some builds ignore extras)\n",
    "os.environ[\"OPENCV_FFMPEG_CAPTURE_OPTIONS\"] = \"rtsp_transport;tcp|max_delay;0\"\n",
    "\n",
    "# =========================\n",
    "# ROI mask (normalized polygon)\n",
    "# =========================\n",
    "USE_MASK = True\n",
    "MASK_POLYGONS = [[\n",
    "    (0.0, 1.0),   # bottom-left\n",
    "    (0.85, 1.0),  # bottom-right\n",
    "    (0.98, 0.4),  # top-right\n",
    "    (0.7, 0.35),\n",
    "    (0.0, 0.57)   # top-left\n",
    "]]\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "\n",
    "def ensure_dir(p):\n",
    "    if not os.path.isdir(p):\n",
    "        os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def timestamp_str(ts=None):\n",
    "    if ts is None: ts = time.time()\n",
    "    return datetime.fromtimestamp(ts).strftime(\"%Y%m%d_%H%M%S_%f\")[:-3]\n",
    "\n",
    "def make_mask(shape):\n",
    "    \"\"\"Return uint8 mask {0,255} for given frame shape using normalized polygon(s).\"\"\"\n",
    "    if not USE_MASK or not MASK_POLYGONS:\n",
    "        return np.full(shape[:2], 255, dtype=np.uint8)\n",
    "    h, w = shape[:2]\n",
    "    m = np.zeros((h, w), dtype=np.uint8)\n",
    "    for poly in MASK_POLYGONS:\n",
    "        pts = np.array([[int(x*w), int(y*h)] for x, y in poly], dtype=np.int32)\n",
    "        cv2.fillPoly(m, [pts], 255)\n",
    "    return m\n",
    "\n",
    "def grab_latest_nonblocking(cap, max_ms=8, max_grabs=30):\n",
    "    \"\"\"Drain queued frames fast, then retrieve newest without blocking too long.\"\"\"\n",
    "    t0 = time.time()\n",
    "    grabs = 0\n",
    "    while grabs < max_grabs:\n",
    "        if not cap.grab():  # nothing ready\n",
    "            break\n",
    "        grabs += 1\n",
    "        if (time.time() - t0) * 1000.0 >= max_ms:\n",
    "            break\n",
    "    ok, frame = cap.retrieve()\n",
    "    if not ok or frame is None:\n",
    "        ok, frame = cap.read()\n",
    "    return ok, frame\n",
    "\n",
    "def encode_jpeg(img, quality=60):\n",
    "    ok, buf = cv2.imencode(\".jpg\", img, [int(cv2.IMWRITE_JPEG_QUALITY), quality])\n",
    "    return buf.tobytes() if ok else None\n",
    "\n",
    "def merge_contours_to_bbox(contours, min_area=150):\n",
    "    \"\"\"Return a single union bbox (x,y,w,h) for all contours above min_area; None if none.\"\"\"\n",
    "    boxes = []\n",
    "    for c in contours:\n",
    "        if cv2.contourArea(c) > min_area:\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            boxes.append((x, y, x+w, y+h))\n",
    "    if not boxes:\n",
    "        return None\n",
    "    x1 = min(b[0] for b in boxes)\n",
    "    y1 = min(b[1] for b in boxes)\n",
    "    x2 = max(b[2] for b in boxes)\n",
    "    y2 = max(b[3] for b in boxes)\n",
    "    return (x1, y1, x2 - x1, y2 - y1)\n",
    "\n",
    "def pad_and_clip_rect(x, y, w, h, pad_px, W, H):\n",
    "    x = max(0, int(x - pad_px)); y = max(0, int(y - pad_px))\n",
    "    w = int(w + 2*pad_px);       h = int(h + 2*pad_px)\n",
    "    x2 = min(W, x + w);          y2 = min(H, y + h)\n",
    "    return x, y, x2 - x, y2 - y\n",
    "\n",
    "def choose_best_by_time(history, target_ts, max_age=None):\n",
    "    \"\"\"\n",
    "    history: deque of (ts, frame). Returns (ts, frame) with |ts-target_ts| minimal.\n",
    "    If max_age provided (seconds), require |dt| <= max_age else return None.\n",
    "    \"\"\"\n",
    "    best = None; best_dt = 1e9\n",
    "    for ts, f in history:\n",
    "        dt = abs(ts - target_ts)\n",
    "        if dt < best_dt:\n",
    "            best_dt = dt\n",
    "            best = (ts, f)\n",
    "    if best is None:\n",
    "        return None\n",
    "    if (max_age is not None) and (best_dt > max_age):\n",
    "        return None\n",
    "    return best\n",
    "\n",
    "def update_display(handle, frame_bgr, mask_bin, roi_mask):\n",
    "    \"\"\"Lean UI: draw boxes & show low-res binary side-by-side (no big previews).\"\"\"\n",
    "    vis = frame_bgr.copy()\n",
    "    if roi_mask is not None:\n",
    "        inv = cv2.bitwise_not(roi_mask)\n",
    "        dim = (vis * 0.35).astype(np.uint8)\n",
    "        vis = cv2.add(cv2.bitwise_and(dim, dim, mask=inv),\n",
    "                      cv2.bitwise_and(vis, vis, mask=roi_mask))\n",
    "    contours, _ = cv2.findContours(mask_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for c in contours:\n",
    "        if cv2.contourArea(c) > 600:\n",
    "            x, y, w, h = cv2.boundingRect(c)\n",
    "            cv2.rectangle(vis, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "    right = cv2.cvtColor(mask_bin, cv2.COLOR_GRAY2BGR)\n",
    "    panel = np.hstack([vis, right])\n",
    "    data = encode_jpeg(panel, quality=55)  # smaller UI payload\n",
    "    if data is not None:\n",
    "        handle.update(IPyImage(data=data))\n",
    "\n",
    "# =========================\n",
    "# Background HQ sampler (non-blocking)\n",
    "# =========================\n",
    "class HQSampler:\n",
    "    def __init__(self, rtsp_url):\n",
    "        self.rtsp = rtsp_url\n",
    "        self.cap = cv2.VideoCapture(self.rtsp, cv2.CAP_FFMPEG)\n",
    "        if not self.cap.isOpened():\n",
    "            raise RuntimeError(\"Could not open HQ RTSP stream\")\n",
    "        try: self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "        except Exception: pass\n",
    "        self.latest = (0.0, None)   # (ts, frame)\n",
    "        self.alive = True\n",
    "        self.lock = threading.Lock()\n",
    "        self.t = threading.Thread(target=self._loop, daemon=True)\n",
    "        self.t.start()\n",
    "\n",
    "    def _loop(self):\n",
    "        while self.alive:\n",
    "            ok, frame = grab_latest_nonblocking(self.cap, max_ms=8, max_grabs=40)\n",
    "            if ok and frame is not None:\n",
    "                with self.lock:\n",
    "                    self.latest = (time.time(), frame)\n",
    "            else:\n",
    "                # attempt quick reopen\n",
    "                try: self.cap.release()\n",
    "                except: pass\n",
    "                time.sleep(0.25)\n",
    "                self.cap = cv2.VideoCapture(self.rtsp, cv2.CAP_FFMPEG)\n",
    "                try: self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "                except: pass\n",
    "\n",
    "    def get_latest(self, max_age=1.5):\n",
    "        with self.lock:\n",
    "            ts, fr = self.latest\n",
    "        if fr is None or (time.time() - ts) > max_age:\n",
    "            return None\n",
    "        return fr\n",
    "\n",
    "    def stop(self):\n",
    "        self.alive = False\n",
    "        try: self.t.join(timeout=0.5)\n",
    "        except: pass\n",
    "        try: self.cap.release()\n",
    "        except: pass\n",
    "\n",
    "# =========================\n",
    "# Async file writer (non-blocking saves)\n",
    "# =========================\n",
    "class FileWriter:\n",
    "    def __init__(self, maxsize=32):\n",
    "        self.q = Queue(maxsize=maxsize)\n",
    "        self.t = threading.Thread(target=self._loop, daemon=True)\n",
    "        self.t.start()\n",
    "\n",
    "    def _loop(self):\n",
    "        while True:\n",
    "            item = self.q.get()\n",
    "            if item is None: break\n",
    "            path, img, quality = item\n",
    "            try:\n",
    "                cv2.imwrite(path, img, [int(cv2.IMWRITE_JPEG_QUALITY), int(quality)])\n",
    "            except Exception:\n",
    "                pass\n",
    "            self.q.task_done()\n",
    "\n",
    "    def save_jpg(self, path, img, quality=82):\n",
    "        try:\n",
    "            self.q.put_nowait((path, img, quality))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def stop(self):\n",
    "        try: self.q.put(None)\n",
    "        except: pass\n",
    "        try: self.t.join(timeout=1.0)\n",
    "        except: pass\n",
    "\n",
    "\n",
    "# ===== AI functions ======\n",
    "\n",
    "# This new function takes an image frame (NumPy array) instead of a file path.\n",
    "def score_frame_against_prompts(frame, prompts, processor, model, device):\n",
    "    \"\"\"\n",
    "    Takes an OpenCV frame (NumPy array), runs it through the CLIP model,\n",
    "    and returns the probabilities for each prompt.\n",
    "    \"\"\"\n",
    "    # 1. Convert the color space from OpenCV's BGR to the standard RGB.\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 2. Convert the NumPy array to a Pillow Image object.\n",
    "    image = Image.fromarray(rgb_frame)\n",
    "    \n",
    "    # --- The rest of the logic is the same as your original function ---\n",
    "    \n",
    "    # Preprocess the text and image.\n",
    "    inputs = processor(text=prompts, images=image, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Move the input tensors to the selected device (GPU or CPU).\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Perform inference.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        # The softmax function converts scores into probabilities.\n",
    "        probs = logits_per_image.softmax(dim=1).squeeze().cpu().numpy()\n",
    "        \n",
    "    return probs\n",
    "\n",
    "# =========================\n",
    "# Main runner\n",
    "# =========================\n",
    "def run_motion_view(display_width=960, ui_fps=2, lookback_secs=10.0,\n",
    "                    save_dir=\"motion_events\", jpg_quality=82,\n",
    "                    event_cooldown=2.0):\n",
    "    \"\"\"\n",
    "    Detect motion on LOW stream (current vs LOW ~lookback_secs ago).\n",
    "    On motion, crop LOW-then (low-res frame around t-10s) and HQ-now (background sampler),\n",
    "    using the same bbox mapped to HQ.\n",
    "    \"\"\"\n",
    "    ensure_dir(save_dir)\n",
    "\n",
    "    highest_fox_score = 0\n",
    "\n",
    "    # Open LOW stream for detection\n",
    "    cap_low = cv2.VideoCapture(RTSP_LOW, cv2.CAP_FFMPEG)\n",
    "    if not cap_low.isOpened():\n",
    "        raise RuntimeError(\"Could not open LOW RTSP stream\")\n",
    "    try: cap_low.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "    except Exception: pass\n",
    "\n",
    "    # Background HQ sampler (kept warm)\n",
    "    hq = HQSampler(RTSP_HIGH)\n",
    "\n",
    "    # Async file writer\n",
    "    fw = FileWriter()\n",
    "\n",
    "    # Display handle\n",
    "    dummy = np.zeros((1,1,3), dtype=np.uint8)\n",
    "    _, buf = cv2.imencode(\".jpg\", dummy)\n",
    "    handle = display(IPyImage(data=buf.tobytes()), display_id=True)\n",
    "\n",
    "    roi_mask_low = None\n",
    "\n",
    "    # Histories (LOW)\n",
    "    hist_len = int(max(lookback_secs * ui_fps * 2, 40))\n",
    "    low_gray_history  = deque(maxlen=hist_len)   # (ts, gray_roi_low)\n",
    "    low_color_history = deque(maxlen=hist_len)   # (ts, full_color_low)\n",
    "\n",
    "    interval = 1.0 / float(max(1, ui_fps))\n",
    "    next_tick = time.time()\n",
    "\n",
    "    # Event throttle\n",
    "    last_event_ts = 0.0\n",
    "    print(f\"Streaming at ~{ui_fps} fps; HQ-now vs LOW-then (~{lookback_secs}s) crops on motion‚Ä¶ (interrupt to stop)\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            now = time.time()\n",
    "            # tick pacing\n",
    "            if now < next_tick:\n",
    "                time.sleep(max(0, next_tick - now))\n",
    "                continue\n",
    "            next_tick += interval\n",
    "\n",
    "            # --- LOW: grab newest quickly\n",
    "            ok_low, frame_low = grab_latest_nonblocking(cap_low, max_ms=8, max_grabs=40)\n",
    "            if not ok_low or frame_low is None:\n",
    "                try: cap_low.release()\n",
    "                except Exception: pass\n",
    "                time.sleep(0.2)\n",
    "                cap_low = cv2.VideoCapture(RTSP_LOW, cv2.CAP_FFMPEG)\n",
    "                try: cap_low.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "                except Exception: pass\n",
    "                continue\n",
    "\n",
    "            # Resize LOW for consistent processing / UI width\n",
    "            if display_width and frame_low.shape[1] > display_width:\n",
    "                h = int(frame_low.shape[0] * (display_width / float(frame_low.shape[1])))\n",
    "                frame_low = cv2.resize(frame_low, (display_width, h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # ROI mask per current LOW resolution\n",
    "            if roi_mask_low is None:\n",
    "                roi_mask_low = make_mask(frame_low.shape)\n",
    "\n",
    "            # Build grayscale ROI for motion\n",
    "            gray = cv2.cvtColor(frame_low, cv2.COLOR_BGR2GRAY)\n",
    "            gray = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "            gray_roi = cv2.bitwise_and(gray, gray, mask=roi_mask_low)\n",
    "\n",
    "            # Reference LOW ~lookback_secs ago (for diff + crop-then)\n",
    "            ref_ts_target = now - lookback_secs\n",
    "            ref_low_gray = choose_best_by_time(low_gray_history,  ref_ts_target)\n",
    "            ref_low_col  = choose_best_by_time(low_color_history, ref_ts_target, max_age=3.0)\n",
    "\n",
    "            if ref_low_gray is None or ref_low_col is None:\n",
    "                # Not enough history yet; push and show empty motion\n",
    "                low_gray_history.append((now, gray_roi.copy()))\n",
    "                low_color_history.append((now, frame_low.copy()))\n",
    "                empty = np.zeros_like(gray_roi, dtype=np.uint8)\n",
    "                update_display(handle, frame_low, empty, roi_mask_low)\n",
    "                continue\n",
    "            _, ref_gray = ref_low_gray\n",
    "            _, low_then_frame = ref_low_col\n",
    "\n",
    "            # --- Motion mask (LOW): keep it light\n",
    "            diff = cv2.absdiff(gray_roi, ref_gray)\n",
    "            _, motion_bin = cv2.threshold(diff, 12, 255, cv2.THRESH_BINARY)\n",
    "            motion_bin = cv2.morphologyEx(motion_bin, cv2.MORPH_OPEN,\n",
    "                                          np.ones((3,3), np.uint8), iterations=1)\n",
    "            motion_bin = cv2.dilate(motion_bin, None, iterations=1)  # lighter than 3\n",
    "\n",
    "            # Push current LOW frames to histories (after computing diff)\n",
    "            low_gray_history.append((now, gray_roi.copy()))\n",
    "            low_color_history.append((now, frame_low.copy()))\n",
    "\n",
    "            # UI update (LOW only)\n",
    "            update_display(handle, frame_low, motion_bin, roi_mask_low)\n",
    "\n",
    "            # --- If motion detected, crop/save (cooldown + non-blocking HQ + async writes)\n",
    "            contours, _ = cv2.findContours(motion_bin, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            union_bbox_low = merge_contours_to_bbox(contours, min_area=100)\n",
    "\n",
    "            if union_bbox_low is None:\n",
    "                continue\n",
    "\n",
    "            # Cooldown to avoid floods on continuous motion\n",
    "            if (now - last_event_ts) < float(event_cooldown):\n",
    "                continue\n",
    "            last_event_ts = now\n",
    "\n",
    "            # Get a recent HQ frame instantly (from sampler)\n",
    "            hq_now = hq.get_latest(max_age=1.5)\n",
    "            if hq_now is None:\n",
    "                # No fresh HQ available‚Äîskip saving this event to avoid blocking\n",
    "                continue\n",
    "\n",
    "            # Map LOW bbox to HQ coords\n",
    "            x, y, w, h = union_bbox_low\n",
    "            Hl, Wl = frame_low.shape[:2]\n",
    "            Hh, Wh = hq_now.shape[:2]\n",
    "            sx = Wh / float(Wl)\n",
    "            sy = Hh / float(Hl)\n",
    "\n",
    "            x_hq = int(x * sx); y_hq = int(y * sy)\n",
    "            w_hq = int(w * sx); h_hq = int(h * sy)\n",
    "\n",
    "            # Pad & clip both\n",
    "            pad_hq = int(0.07 * max(w_hq, h_hq)) + 8\n",
    "            x_hq, y_hq, w_hq, h_hq = pad_and_clip_rect(x_hq, y_hq, w_hq, h_hq, pad_hq, Wh, Hh)\n",
    "\n",
    "            pad_low = int(0.07 * max(w, h)) + 4\n",
    "            xL, yL, wL, hL = pad_and_clip_rect(x, y, w, h, pad_low,\n",
    "                                               frame_low.shape[1], frame_low.shape[0])\n",
    "\n",
    "            # Extract crops\n",
    "            crop_hq_now   = hq_now[y_hq:y_hq+h_hq, x_hq:x_hq+w_hq]\n",
    "            crop_low_then = low_then_frame[yL:yL+hL, xL:xL+wL]\n",
    "\n",
    "            # --- Inside your main loop, after a motion bounding box is found ---\n",
    "\n",
    "            # ... (previous code to get hq_now and union_bbox_low) ...\n",
    "\n",
    "            # Extract the high-quality crop\n",
    "            crop_hq_now = hq_now[y_hq:y_hq+h_hq, x_hq:x_hq+w_hq]\n",
    "\n",
    "            # >>> ADD THIS NEW BLOCK FOR LIVE SCORING <<<\n",
    "\n",
    "            # Score the cropped image against your prompts in memory\n",
    "            probabilities = score_frame_against_prompts(crop_hq_now, PROMPTS, processor, model, DEVICE)\n",
    "            \n",
    "            # Find the score specifically for the \"fox\" prompt\n",
    "            # Note: The index must match the position in your PROMPTS list\n",
    "            fox_index = PROMPTS.index(\"a photograph of a fox\")\n",
    "            fox_score = probabilities[fox_index]\n",
    "\n",
    "            decoys = np.sum(probabilities[:n_people_decoys])\n",
    "\n",
    "            print(f\"fox: {fox_score}, people_sum: {decoys}, diff: {fox_score - decoys}\")\n",
    "\n",
    "            if fox_score > highest_fox_score:\n",
    "                print(fox_score)\n",
    "                highest_fox_score = fox_score\n",
    "\n",
    "            # for thing, prob in zip(PROMPTS, probabilities):\n",
    "            #     print(f\"{thing} {prob}\")\n",
    "\n",
    "            # For debugging, print the score\n",
    "            print(f\"Fox score: {fox_score:.2f}\")\n",
    "\n",
    "            # If the score is above your threshold, take action\n",
    "            if fox_score > FOX_SCORE_THRESHOLD and decoys < people_sum_thresh:\n",
    "                print(f\"ü¶ä Fox detected with confidence {fox_score:.2f}! Triggering water valve... üíß\")\n",
    "                \n",
    "                #\n",
    "                # --- YOUR HARDWARE TRIGGER LOGIC GOES HERE ---\n",
    "                # For example: trigger_water_valve()\n",
    "                #\n",
    "                \n",
    "                # Save the image of the detected fox for review\n",
    "                ev_dir = os.path.join(save_dir, timestamp_str(now))\n",
    "                ensure_dir(ev_dir)\n",
    "                # You can give it a special name to easily find it later\n",
    "                fw.save_jpg(os.path.join(ev_dir, f\"fox_detected_{fox_score:.2f}.jpg\"), crop_hq_now, jpg_quality)\n",
    "\n",
    "            \n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopped.\")\n",
    "    finally:\n",
    "        try: cap_low.release()\n",
    "        except: pass\n",
    "        try: hq.stop()\n",
    "        except: pass\n",
    "        try: fw.stop()\n",
    "        except: pass\n",
    "\n",
    "# =========================\n",
    "# Run it\n",
    "# =========================\n",
    "# ui_fps controls how often a frame is processed & how densely the LOW history is sampled.\n",
    "# lookback_secs is the comparison window (LOW ~10s ago vs HQ now).\n",
    "run_motion_view(\n",
    "    ui_fps=1,\n",
    "    lookback_secs=10.0,\n",
    "    save_dir=\"motion_events\",\n",
    "    jpg_quality=82,\n",
    "    event_cooldown=0.5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aaa59e-1bee-4b3b-8ceb-8480ca85bf8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fox2)",
   "language": "python",
   "name": "fox2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
